{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jiangzhx/colab/blob/main/Quickstart_LLM_Fine_Tuning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "72g9lk8nEdaa"
      },
      "source": [
        "# **Quickstart: LLM Fine-Tuning with Predibase**\n",
        "\n",
        "This quickstart will show you how to prompt, fine-tune, and deploy LLMs in Predibase. We'll be following a code generation use case where our end result will be a fine-tuned Llama 2 7B model that takes in natural language as input and returns code as output."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mEfKhgnpA9sS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "68636dfd-8bba-40c0-b7e8-7609b3cab80a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/59.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.6/59.6 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.4/12.4 MB\u001b[0m \u001b[31m43.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.9/129.9 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m64.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.1/20.1 MB\u001b[0m \u001b[31m56.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m54.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m100.3/100.3 kB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.4/6.4 MB\u001b[0m \u001b[31m62.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m64.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m247.1/247.1 kB\u001b[0m \u001b[31m22.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for predibase (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for predibase-api (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for progress-table (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "pip install -U predibase --quiet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TkGcwkRZBWTl"
      },
      "source": [
        "# **Setup**\n",
        "\n",
        "You'll first need to initialize your PredibaseClient object and configure your API token."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "POjTWL2EArrT",
        "outputId": "5c099e76-d4e3-469c-e5b4-399363de7856"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">Connected to Predibase as </span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">User</span><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">id</span><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">=</span><span style=\"color: #ffff00; text-decoration-color: #ffff00\">1ba4cbdf</span><span style=\"color: #ffff00; text-decoration-color: #ffff00\">-c1e8-4407-a707-6ecdcff7bce1</span><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">, </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">username</span><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">=</span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">jackie</span><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">)</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1;34mConnected to Predibase as \u001b[0m\u001b[1;35mUser\u001b[0m\u001b[1;34m(\u001b[0m\u001b[1;33mid\u001b[0m\u001b[1;34m=\u001b[0m\u001b[93m1ba4cbdf\u001b[0m\u001b[93m-c1e8-4407-a707-6ecdcff7bce1\u001b[0m\u001b[1;34m, \u001b[0m\u001b[1;33musername\u001b[0m\u001b[1;34m=\u001b[0m\u001b[1;35mjackie\u001b[0m\u001b[1;34m)\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from predibase import PredibaseClient\n",
        "\n",
        "pc = PredibaseClient(token=\"{your-api-token}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AicnArh2CNgI"
      },
      "source": [
        "# **Prompt a deployed LLM**\n",
        "\n",
        "For our code generation use case, let's first see how Llama 2 7B performs out of the box.\n",
        "\n",
        "If you are in the Predibase SaaS environment, you have access to shared [serverless LLM deployments](https://docs.predibase.com/ui-guide/llms/query-llm/shared_deployments), including Llama 2 7B.\n",
        "\n",
        "If you are in a VPC environment, you'll need to first [deploy a pretrained LLM](https://docs.predibase.com/user-guide/inference/dedicated_deployments#pretrained-llm-deployment)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QRP93WAYCM5C",
        "outputId": "37ae7c15-fa13-4aa6-da12-83dc689020a3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "    ### Instruction: Write an algorithm in Java to reverse the words in a string.\n",
            "\n",
            "    ### Input: The quick brown fox\n",
            "\n",
            "    ### Response:\n",
            "\n",
            "    ### Instruction: Write an algorithm in Java to reverse the words in a string.\n",
            "\n",
            "    ### Input: The quick brown fox\n",
            "\n",
            "    ### Response:\n",
            "\n",
            "    ### Instruction: Write an algorithm in Java to reverse the words in a string.\n",
            "\n",
            "    ### Input: The quick brown fox\n",
            "\n",
            "    ### Response:\n",
            "\n",
            "    ### Instruction: Write an algorithm in Java to reverse the words in a string.\n",
            "\n",
            "    ### Input: The quick brown fox\n",
            "\n",
            "    ### Response:\n",
            "\n",
            "    ### Instruction: Write an algorithm in Java to reverse the words in a string.\n",
            "\n",
            "    ### Input: The quick brown fox\n",
            "\n",
            "    ### Response:\n",
            "\n",
            "    ### Instruction: Write an algorithm in Java to reverse the words in a string.\n",
            "\n",
            "    ### Input: The quick brown fox\n",
            "\n",
            "    ### Response:\n",
            "\n",
            "    ### Instruction: Write an algorithm in Java to reverse the words in a string.\n",
            "\n",
            "    ### Input: The quick brown fox\n",
            "\n",
            "    ###\n"
          ]
        }
      ],
      "source": [
        "llm_deployment = pc.LLM(\"pb://deployments/llama-2-7b\")\n",
        "result: list = llm_deployment.prompt(\"\"\"\n",
        "    Below is an instruction that describes a task, paired with an input\n",
        "    that may provide further context. Write a response that appropriately\n",
        "    completes the request.\n",
        "\n",
        "    ### Instruction: Write an algorithm in Java to reverse the words in a string.\n",
        "\n",
        "    ### Input: The quick brown fox\n",
        "\n",
        "    ### Response:\n",
        "\"\"\", max_new_tokens=256)\n",
        "print(result.response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "li4VqIw4CVp2"
      },
      "source": [
        "# **Fine-tune a pretrained LLM**\n",
        "\n",
        "Next we'll upload a dataset and fine-tune to see if we can get better performance.\n",
        "\n",
        "The [Code Alpaca](https://github.com/sahil280114/codealpaca) dataset is used for fine-tuning large language models to follow instructions to produce code from natural language and consists of the following columns:\n",
        "\n",
        "- `instruction` that describes a task\n",
        "- `input` when additional context is required for the instruction\n",
        "- the expected `output`\n",
        "\n",
        "\n",
        "For the sake of this quickstart, we've created a version of the Code Alpaca dataset with fewer rows so that the model trains significantly faster."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c9ZKc5kmCac3",
        "outputId": "336168d7-585e-4c26-a1c7-2c1046d37901"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2023-10-06 20:55:05--  https://predibase-public-us-west-2.s3.us-west-2.amazonaws.com/datasets/code_alpaca_800.csv\n",
            "Resolving predibase-public-us-west-2.s3.us-west-2.amazonaws.com (predibase-public-us-west-2.s3.us-west-2.amazonaws.com)... 52.92.152.234, 52.218.182.242, 52.218.221.121, ...\n",
            "Connecting to predibase-public-us-west-2.s3.us-west-2.amazonaws.com (predibase-public-us-west-2.s3.us-west-2.amazonaws.com)|52.92.152.234|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 234707 (229K) [text/csv]\n",
            "Saving to: ‘code_alpaca_800.csv’\n",
            "\n",
            "code_alpaca_800.csv 100%[===================>] 229.21K  --.-KB/s    in 0.1s    \n",
            "\n",
            "2023-10-06 20:55:05 (2.06 MB/s) - ‘code_alpaca_800.csv’ saved [234707/234707]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://predibase-public-us-west-2.s3.us-west-2.amazonaws.com/datasets/code_alpaca_800.csv"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LDMT0IdfWmIN"
      },
      "source": [
        "**Now we will perform the following actions to start our fine-tuning job:**\n",
        "1. Upload the dataset to Predibase for training\n",
        "2. Create a prompt template to use for fine-tuning\n",
        "3. Select the LLM we want to fine-tune\n",
        "4. Kick off the fine-tuning job\n",
        "\n",
        "The fine-tuning job should take around 35-45 minutes total. Queueing time depends on how quickly we're able acquire resources and what other jobs might be ahead in the queue. The training time itself should be around 25-30 minutes. As the model trains, you can receive updated metrics in your notebook or terminal. You can also see metrics and visualizations in the Predibase UI."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-zxX9nfv0Rqc"
      },
      "outputs": [],
      "source": [
        "# Upload the dataset to Predibase (estimated time: 2 minutes due to creation of Predibase dataset with dataset profile)\n",
        "# If you've already uploaded the dataset before, you can skip uploading and get the dataset directly with\n",
        "# \"dataset = pc.get_dataset(\"code_alpaca_800\", \"file_uploads\")\".\n",
        "dataset = pc.upload_dataset(\"code_alpaca_800.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "JKAPXD_VCbNd",
        "outputId": "ad2aee46-d46e-49a6-859d-e7bc83a36688"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1;34mCreated model repository: \u001b[0m\u001b[1;34m<\u001b[0m\u001b[1;95mLlama-\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;95m-7b-hf-code_alpaca_800\u001b[0m\u001b[1;34m>\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">Created model repository: &lt;</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">Llama-</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">-7b-hf-code_alpaca_800</span><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">&gt;</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1;34mCheck Status of Model Training Here: \u001b[0m\u001b]8;id=414393;https://app.predibase.com/models/version/4311\u001b\\\u001b[4;94mhttps://app.predibase.com/models/version/4311\u001b[0m\u001b]8;;\u001b\\\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">Check Status of Model Training Here: </span><a href=\"https://app.predibase.com/models/version/4311\" target=\"_blank\"><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://app.predibase.com/models/version/4311</span></a>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1;34mMonitoring status of model training\u001b[0m\u001b[1;33m...\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">Monitoring status of model training</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">...</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1;34mCompute summary:\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">Compute summary:</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1;34m  Cloud: aws\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">  Cloud: aws</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1;34m  * g4dn.2xlarge \u001b[0m\u001b[1;34m(\u001b[0m\u001b[1;34mx1\u001b[0m\u001b[1;34m)\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">  * g4dn.2xlarge (x1)</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "✓ Queued 00:07:59   \n",
            "✓ Preprocessing 00:08:09   \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "┌──────────┬──────────┬──────────────────┬──────────────────────────┬──────────┬──────────┬──────────┐\n",
            "│  epochs  \u001b[0m│   time   \u001b[0m│     feature      \u001b[0m│          metric          \u001b[0m│  train   \u001b[0m│   val    \u001b[0m│   test   \u001b[0m│\n",
            "├──────────┼──────────┼──────────────────┼──────────────────────────┼──────────┼──────────┼──────────┤\n",
            "│    1     \u001b[0m│ 00:14:22 \u001b[0m│     combined     \u001b[0m│           loss           \u001b[0m│  0.8729  \u001b[0m│          \u001b[0m│  1.5568  \u001b[0m│\n",
            "│          \u001b[0m│          \u001b[0m│      output      \u001b[0m│           bleu           \u001b[0m│  0.1988  \u001b[0m│          \u001b[0m│  0.1616  \u001b[0m│\n",
            "│          \u001b[0m│          \u001b[0m│                  \u001b[0m│     char_error_rate      \u001b[0m│  1.3440  \u001b[0m│          \u001b[0m│  1.3493  \u001b[0m│\n",
            "│          \u001b[0m│          \u001b[0m│                  \u001b[0m│           loss           \u001b[0m│  0.8729  \u001b[0m│          \u001b[0m│  1.5568  \u001b[0m│\n",
            "│          \u001b[0m│          \u001b[0m│                  \u001b[0m│  next_token_perplexity   \u001b[0m│ 15032.72…\u001b[0m│          \u001b[0m│ 17560.66…\u001b[0m│\n",
            "│          \u001b[0m│          \u001b[0m│                  \u001b[0m│        perplexity        \u001b[0m│ 31766.42…\u001b[0m│          \u001b[0m│ 31745.89…\u001b[0m│\n",
            "│          \u001b[0m│          \u001b[0m│                  \u001b[0m│     rouge1_fmeasure      \u001b[0m│  0.4335  \u001b[0m│          \u001b[0m│  0.3470  \u001b[0m│\n",
            "│          \u001b[0m│          \u001b[0m│                  \u001b[0m│     rouge1_precision     \u001b[0m│  0.3090  \u001b[0m│          \u001b[0m│  0.2578  \u001b[0m│\n",
            "│          \u001b[0m│          \u001b[0m│                  \u001b[0m│      rouge1_recall       \u001b[0m│  0.8326  \u001b[0m│          \u001b[0m│  0.6193  \u001b[0m│\n",
            "│          \u001b[0m│          \u001b[0m│                  \u001b[0m│     rouge2_fmeasure      \u001b[0m│  0.3253  \u001b[0m│          \u001b[0m│  0.2022  \u001b[0m│\n",
            "│          \u001b[0m│          \u001b[0m│                  \u001b[0m│     rouge2_precision     \u001b[0m│  0.2305  \u001b[0m│          \u001b[0m│  0.1508  \u001b[0m│\n",
            "│          \u001b[0m│          \u001b[0m│                  \u001b[0m│      rouge2_recall       \u001b[0m│  0.6378  \u001b[0m│          \u001b[0m│  0.3583  \u001b[0m│\n",
            "│          \u001b[0m│          \u001b[0m│                  \u001b[0m│     rougeL_fmeasure      \u001b[0m│  0.4077  \u001b[0m│          \u001b[0m│  0.3068  \u001b[0m│\n",
            "│          \u001b[0m│          \u001b[0m│                  \u001b[0m│     rougeL_precision     \u001b[0m│  0.2896  \u001b[0m│          \u001b[0m│  0.2270  \u001b[0m│\n",
            "│          \u001b[0m│          \u001b[0m│                  \u001b[0m│      rougeL_recall       \u001b[0m│  0.7918  \u001b[0m│          \u001b[0m│  0.5553  \u001b[0m│\n",
            "│          \u001b[0m│          \u001b[0m│                  \u001b[0m│    rougeLsum_fmeasure    \u001b[0m│  0.4174  \u001b[0m│          \u001b[0m│  0.3175  \u001b[0m│\n",
            "│          \u001b[0m│          \u001b[0m│                  \u001b[0m│   rougeLsum_precision    \u001b[0m│  0.2970  \u001b[0m│          \u001b[0m│  0.2358  \u001b[0m│\n",
            "│          \u001b[0m│          \u001b[0m│                  \u001b[0m│     rougeLsum_recall     \u001b[0m│  0.8069  \u001b[0m│          \u001b[0m│  0.5692  \u001b[0m│\n",
            "│          \u001b[0m│          \u001b[0m│                  \u001b[0m│    sequence_accuracy     \u001b[0m│    0     \u001b[0m│          \u001b[0m│    0     \u001b[0m│\n",
            "│          \u001b[0m│          \u001b[0m│                  \u001b[0m│      token_accuracy      \u001b[0m│  0.0030  \u001b[0m│          \u001b[0m│  0.0042  \u001b[0m│\n",
            "│          \u001b[0m│          \u001b[0m│                  \u001b[0m│     word_error_rate      \u001b[0m│  1.8001  \u001b[0m│          \u001b[0m│  1.7623  \u001b[0m│\n",
            "│    2     \u001b[0m│ 00:25:58 \u001b[0m│     combined     \u001b[0m│           loss           \u001b[0m│  0.9662  \u001b[0m│          \u001b[0m│  1.3997  \u001b[0m│\n",
            "│          \u001b[0m│          \u001b[0m│      output      \u001b[0m│           bleu           \u001b[0m│  0.1787  \u001b[0m│          \u001b[0m│  0.1662  \u001b[0m│\n",
            "│          \u001b[0m│          \u001b[0m│                  \u001b[0m│     char_error_rate      \u001b[0m│  1.6179  \u001b[0m│          \u001b[0m│  1.3653  \u001b[0m│\n",
            "│          \u001b[0m│          \u001b[0m│                  \u001b[0m│           loss           \u001b[0m│  0.9662  \u001b[0m│          \u001b[0m│  1.3997  \u001b[0m│\n",
            "│          \u001b[0m│          \u001b[0m│                  \u001b[0m│  next_token_perplexity   \u001b[0m│ 15159.46…\u001b[0m│          \u001b[0m│ 17203.90…\u001b[0m│\n",
            "│          \u001b[0m│          \u001b[0m│                  \u001b[0m│        perplexity        \u001b[0m│ 31528.68…\u001b[0m│          \u001b[0m│ 31796.24…\u001b[0m│\n",
            "│          \u001b[0m│          \u001b[0m│                  \u001b[0m│     rouge1_fmeasure      \u001b[0m│  0.3661  \u001b[0m│          \u001b[0m│  0.3525  \u001b[0m│\n",
            "│          \u001b[0m│          \u001b[0m│                  \u001b[0m│     rouge1_precision     \u001b[0m│  0.2598  \u001b[0m│          \u001b[0m│  0.2604  \u001b[0m│\n",
            "│          \u001b[0m│          \u001b[0m│                  \u001b[0m│      rouge1_recall       \u001b[0m│  0.7265  \u001b[0m│          \u001b[0m│  0.6433  \u001b[0m│\n",
            "│          \u001b[0m│          \u001b[0m│                  \u001b[0m│     rouge2_fmeasure      \u001b[0m│  0.2855  \u001b[0m│          \u001b[0m│  0.2118  \u001b[0m│\n",
            "│          \u001b[0m│          \u001b[0m│                  \u001b[0m│     rouge2_precision     \u001b[0m│  0.2022  \u001b[0m│          \u001b[0m│  0.1566  \u001b[0m│\n",
            "│          \u001b[0m│          \u001b[0m│                  \u001b[0m│      rouge2_recall       \u001b[0m│  0.5658  \u001b[0m│          \u001b[0m│  0.3840  \u001b[0m│\n",
            "│          \u001b[0m│          \u001b[0m│                  \u001b[0m│     rougeL_fmeasure      \u001b[0m│  0.3502  \u001b[0m│          \u001b[0m│  0.3115  \u001b[0m│\n",
            "│          \u001b[0m│          \u001b[0m│                  \u001b[0m│     rougeL_precision     \u001b[0m│  0.2480  \u001b[0m│          \u001b[0m│  0.2297  \u001b[0m│\n",
            "│          \u001b[0m│          \u001b[0m│                  \u001b[0m│      rougeL_recall       \u001b[0m│  0.7013  \u001b[0m│          \u001b[0m│  0.5702  \u001b[0m│\n",
            "│          \u001b[0m│          \u001b[0m│                  \u001b[0m│    rougeLsum_fmeasure    \u001b[0m│  0.3528  \u001b[0m│          \u001b[0m│  0.3254  \u001b[0m│\n",
            "│          \u001b[0m│          \u001b[0m│                  \u001b[0m│   rougeLsum_precision    \u001b[0m│  0.2500  \u001b[0m│          \u001b[0m│  0.2405  \u001b[0m│\n",
            "│          \u001b[0m│          \u001b[0m│                  \u001b[0m│     rougeLsum_recall     \u001b[0m│  0.7052  \u001b[0m│          \u001b[0m│  0.5935  \u001b[0m│\n",
            "│          \u001b[0m│          \u001b[0m│                  \u001b[0m│    sequence_accuracy     \u001b[0m│    0     \u001b[0m│          \u001b[0m│    0     \u001b[0m│\n",
            "│          \u001b[0m│          \u001b[0m│                  \u001b[0m│      token_accuracy      \u001b[0m│  0.0080  \u001b[0m│          \u001b[0m│  0.0028  \u001b[0m│\n",
            "│          \u001b[0m│          \u001b[0m│                  \u001b[0m│     word_error_rate      \u001b[0m│  2.1744  \u001b[0m│          \u001b[0m│  1.7721  \u001b[0m│\n",
            "│    3     \u001b[0m│ 00:37:41 \u001b[0m│     combined     \u001b[0m│           loss           \u001b[0m│  0.8118  \u001b[0m│          \u001b[0m│  1.2904  \u001b[0m│\n",
            "│          \u001b[0m│          \u001b[0m│      output      \u001b[0m│           bleu           \u001b[0m│  0.2148  \u001b[0m│          \u001b[0m│  0.1698  \u001b[0m│\n",
            "│          \u001b[0m│          \u001b[0m│                  \u001b[0m│     char_error_rate      \u001b[0m│  1.2799  \u001b[0m│          \u001b[0m│  1.4309  \u001b[0m│\n",
            "│          \u001b[0m│          \u001b[0m│                  \u001b[0m│           loss           \u001b[0m│  0.8118  \u001b[0m│          \u001b[0m│  1.2904  \u001b[0m│\n",
            "│          \u001b[0m│          \u001b[0m│                  \u001b[0m│  next_token_perplexity   \u001b[0m│ 15266.98…\u001b[0m│          \u001b[0m│ 16948.35…\u001b[0m│\n",
            "│          \u001b[0m│          \u001b[0m│                  \u001b[0m│        perplexity        \u001b[0m│ 31504.58…\u001b[0m│          \u001b[0m│ 31783.96…\u001b[0m│\n",
            "│          \u001b[0m│          \u001b[0m│                  \u001b[0m│     rouge1_fmeasure      \u001b[0m│  0.4148  \u001b[0m│          \u001b[0m│  0.3560  \u001b[0m│\n",
            "│          \u001b[0m│          \u001b[0m│                  \u001b[0m│     rouge1_precision     \u001b[0m│  0.3022  \u001b[0m│          \u001b[0m│  0.2636  \u001b[0m│\n",
            "│          \u001b[0m│          \u001b[0m│                  \u001b[0m│      rouge1_recall       \u001b[0m│  0.7816  \u001b[0m│          \u001b[0m│  0.6398  \u001b[0m│\n",
            "│          \u001b[0m│          \u001b[0m│                  \u001b[0m│     rouge2_fmeasure      \u001b[0m│  0.3046  \u001b[0m│          \u001b[0m│  0.2190  \u001b[0m│\n",
            "│          \u001b[0m│          \u001b[0m│                  \u001b[0m│     rouge2_precision     \u001b[0m│  0.2192  \u001b[0m│          \u001b[0m│  0.1619  \u001b[0m│\n",
            "│          \u001b[0m│          \u001b[0m│                  \u001b[0m│      rouge2_recall       \u001b[0m│  0.6064  \u001b[0m│          \u001b[0m│  0.4031  \u001b[0m│\n",
            "│          \u001b[0m│          \u001b[0m│                  \u001b[0m│     rougeL_fmeasure      \u001b[0m│  0.3879  \u001b[0m│          \u001b[0m│  0.3229  \u001b[0m│\n",
            "│          \u001b[0m│          \u001b[0m│                  \u001b[0m│     rougeL_precision     \u001b[0m│  0.2809  \u001b[0m│          \u001b[0m│  0.2382  \u001b[0m│\n",
            "│          \u001b[0m│          \u001b[0m│                  \u001b[0m│      rougeL_recall       \u001b[0m│  0.7424  \u001b[0m│          \u001b[0m│  0.5873  \u001b[0m│\n",
            "│          \u001b[0m│          \u001b[0m│                  \u001b[0m│    rougeLsum_fmeasure    \u001b[0m│  0.3980  \u001b[0m│          \u001b[0m│  0.3328  \u001b[0m│\n",
            "│          \u001b[0m│          \u001b[0m│                  \u001b[0m│   rougeLsum_precision    \u001b[0m│  0.2892  \u001b[0m│          \u001b[0m│  0.2463  \u001b[0m│\n",
            "│          \u001b[0m│          \u001b[0m│                  \u001b[0m│     rougeLsum_recall     \u001b[0m│  0.7558  \u001b[0m│          \u001b[0m│  0.6002  \u001b[0m│\n",
            "│          \u001b[0m│          \u001b[0m│                  \u001b[0m│    sequence_accuracy     \u001b[0m│    0     \u001b[0m│          \u001b[0m│    0     \u001b[0m│\n",
            "│          \u001b[0m│          \u001b[0m│                  \u001b[0m│      token_accuracy      \u001b[0m│  0.0087  \u001b[0m│          \u001b[0m│  0.0027  \u001b[0m│\n",
            "│          \u001b[0m│          \u001b[0m│                  \u001b[0m│     word_error_rate      \u001b[0m│  1.5976  \u001b[0m│          \u001b[0m│  1.7337  \u001b[0m│\n",
            "└──────────┴──────────┴──────────────────┴──────────────────────────┴──────────┴──────────┴──────────┘\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "✓ Evaluating 00:40:59   \n",
            "✓ Visualizing 00:41:01   \n",
            "Ready\n"
          ]
        }
      ],
      "source": [
        "# Define the template used to prompt the model for each example\n",
        "# Note the 4-space indentation, which is necessary for the YAML templating.\n",
        "prompt_template = \"\"\"Below is an instruction that describes a task, paired with an input\n",
        "    that may provide further context. Write a response that appropriately\n",
        "    completes the request.\n",
        "\n",
        "    ### Instruction: {instruction}\n",
        "\n",
        "    ### Input: {input}\n",
        "\n",
        "    ### Response:\n",
        "\"\"\"\n",
        "\n",
        "# Specify the Huggingface LLM you want to fine-tune\n",
        "# Kick off a fine-tuning job on the uploaded dataset\n",
        "llm = pc.LLM(\"hf://meta-llama/Llama-2-7b-hf\")\n",
        "job = llm.finetune(\n",
        "    prompt_template=prompt_template,\n",
        "    target=\"output\",\n",
        "    dataset=dataset,\n",
        "    # repo=\"optional-custom-model-repository-name\"\n",
        ")\n",
        "\n",
        "# Wait for the job to finish and get training updates and metrics\n",
        "model = job.get()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4bSLmVloCl6m"
      },
      "source": [
        "# **Prompt your fine-tuned LLM**\n",
        "\n",
        "Predibase supports both real-time inference, as well as [batch inference](https://docs.predibase.com/user-guide/inference/batch_prediction)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZDeiyLWXr894"
      },
      "source": [
        "#### **Real-time inference using _LoRAX_** (Recommended)\n",
        "\n",
        "[LoRA eXchange (LoRAX)](https://predibase.com/blog/lorax-the-open-source-framework-for-serving-100s-of-fine-tuned-llms-in) allows you to prompt your fine-tuned LLM without needing to create a new deployment for each model you want to prompt. Predibase automatically loads your fine-tuned weights on top of a shared LLM deployment on demand. While this means that there will be a small amount of additional latency, the benefit is that a single LLM deployment can support many different fine-tuned model versions without requiring additional compute.\n",
        "\n",
        "Note: Inference using dynamic adapter deployments is available to both SaaS and VPC users. Predibase provides shared [serverless base LLM deployments](https://docs.predibase.com/user-guide/inference/serverless_deployments) for use in our SaaS environment. VPC users need [deploy their own base model](https://docs.predibase.com/user-guide/inference/dedicated_deployments#pretrained-llm-deployment)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cph4WwqdrSsA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "baf194ec-3db5-4dce-8ed5-ee520b289f01"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   public static String reverseWords(String str) {\n",
            "        String[] words = str.split(\" \");\n",
            "        StringBuilder sb = new StringBuilder();\n",
            "        for (int i = words.length - 1; i >= 0; i--) {\n",
            "            sb.append(words[i]).append(\" \");\n",
            "        }\n",
            "        return sb.toString();\n",
            "    }\n"
          ]
        }
      ],
      "source": [
        "# Since our model was fine-tuned from a Llama-2-7b base, we'll use the shared deployment with the same model type.\n",
        "base_deployment = pc.LLM(\"pb://deployments/llama-2-7b\")\n",
        "\n",
        "# Now we just specify the adapter to use, which is the model we fine-tuned.\n",
        "model = pc.get_model(\"Llama-2-7b-hf-code_alpaca_800\")\n",
        "adapter_deployment = base_deployment.with_adapter(model)\n",
        "\n",
        "# Recall that our model was fine-tuned using a template that accepts an {instruction}\n",
        "# and an {input}. This template is automatically applied when prompting.\n",
        "result = adapter_deployment.prompt(\n",
        "    {\n",
        "      \"instruction\": \"Write an algorithm in Java to reverse the words in a string.\",\n",
        "      \"input\": \"The quick brown fox\"\n",
        "    },\n",
        "    max_new_tokens=256)\n",
        "\n",
        "print(result.response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DPMGvoDyrQqv"
      },
      "source": [
        "#### **Real-time inference using a _Dedicated Deployment_** (VPC and Premium SaaS)\n",
        "\n",
        "Once deployed, you can use the prompt method in the SDK to query your model or use the Query Editor in the Predibase UI. Deploying the fine-tuned LLM from this Quickstart guide should take around 10 minutes.\n",
        "\n",
        "Note: Only **VPC and Premium SaaS users with the Admin role** will be able to deploy a fine-tuned LLM."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vqXyb9BJps_3"
      },
      "outputs": [],
      "source": [
        "finetuned_llm = model.deploy(\"llama-2-7b-finetune\").get()\n",
        "\n",
        "# Recall that our model was fine-tuned using a template that accepts an {instruction}\n",
        "# and an {input}. This template is automatically applied when prompting.\n",
        "result = finetuned_llm.prompt(\n",
        "    {\n",
        "        \"instruction\": \"Write an algorithm in Java to reverse the words in a string.\",\n",
        "        \"input\": \"The quick brown fox\"\n",
        "    },\n",
        "    max_new_tokens=256)\n",
        "\n",
        "print(result.response)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **What's Next?**\n",
        "\n",
        "*   [Advanced fine-tuning customization](https://docs.predibase.com/user-guide/training/finetune#customizing-fine-tuning-with-different-parameter-values-in-line)\n",
        "*   [Prompt via REST](https://docs.predibase.com/user-guide/inference/rest_api)\n"
      ],
      "metadata": {
        "id": "uw14DaynIiZ2"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}